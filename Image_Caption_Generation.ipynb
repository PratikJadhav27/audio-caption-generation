# Image_Caption_Generation.ipynb
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image Captioning System\n",
        "An end-to-end deep learning system that generates textual descriptions for images using CNN-LSTM architecture."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Import Required Libraries"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# TensorFlow/Keras components\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Configuration Settings"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Path configurations\n",
        "BASE_DIR = '/content/drive/MyDrive/Image_Captioning/Flickr8k'\n",
        "WORKING_DIR = '/content/drive/MyDrive/Image_Captioning'\n",
        "\n",
        "# Model parameters\n",
        "MAX_CAPTION_LENGTH = 35\n",
        "VOCAB_SIZE = 8485\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 20"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Feature Extraction with VGG16"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features():\n",
        "    \"\"\"Extract image features using pretrained VGG16 model\"\"\"\n",
        "    # Load pre-trained VGG16\n",
        "    model = VGG16()\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "    \n",
        "    features = {}\n",
        "    image_dir = os.path.join(BASE_DIR, 'Images')\n",
        "    \n",
        "    for img_name in tqdm(os.listdir(image_dir)):\n",
        "        img_path = os.path.join(image_dir, img_name)\n",
        "        image = load_img(img_path, target_size=(224, 224))\n",
        "        image = img_to_array(image)\n",
        "        image = preprocess_input(image.reshape(1, *image.shape))\n",
        "        feature = model.predict(image, verbose=0)\n",
        "        features[img_name.split('.')[0]] = feature\n",
        "    \n",
        "    # Save features\n",
        "    pickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))\n",
        "    return features"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Load and Preprocess Captions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def load_captions():\n",
        "    \"\"\"Load and preprocess image captions\"\"\"\n",
        "    with open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:\n",
        "        captions = f.read().split('\\n')[1:]  # Skip header\n",
        "    \n",
        "    mapping = {}\n",
        "    for line in tqdm(captions):\n",
        "        if ',' not in line:\n",
        "            continue\n",
        "        img_id, caption = line.split(',', 1)\n",
        "        img_id = img_id.split('.')[0]\n",
        "        \n",
        "        # Clean and format caption\n",
        "        caption = caption.lower().replace('[^A-Za-z]', '').replace('\\s+', ' ')\n",
        "        caption = f'startseq {caption.strip()} endseq'\n",
        "        \n",
        "        if img_id not in mapping:\n",
        "            mapping[img_id] = []\n",
        "        mapping[img_id].append(caption)\n",
        "    \n",
        "    return mapping"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model Architecture"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    \"\"\"Create CNN-RNN model architecture\"\"\"\n",
        "    # Image feature extractor\n",
        "    inputs1 = Input(shape=(4096,))\n",
        "    img_features = Dropout(0.4)(inputs1)\n",
        "    img_features = Dense(256, activation='relu')(img_features)\n",
        "\n",
        "    # Sequence processor\n",
        "    inputs2 = Input(shape=(MAX_CAPTION_LENGTH,))\n",
        "    seq_features = Embedding(VOCAB_SIZE, 256, mask_zero=True)(inputs2)\n",
        "    seq_features = Dropout(0.4)(seq_features)\n",
        "    seq_features = LSTM(256)(seq_features)\n",
        "\n",
        "    # Decoder\n",
        "    decoder = add([img_features, seq_features])\n",
        "    decoder = Dense(256, activation='relu')(decoder)\n",
        "    outputs = Dense(VOCAB_SIZE, activation='softmax')(decoder)\n",
        "\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    \n",
        "    plot_model(model, show_shapes=True)\n",
        "    return model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Training Pipeline"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "    \"\"\"Main training function\"\"\"\n",
        "    # Load data\n",
        "    features = pickle.load(open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb'))\n",
        "    mapping = load_captions()\n",
        "    \n",
        "    # Prepare tokenizer\n",
        "    all_captions = [cap for caps in mapping.values() for cap in caps]\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(all_captions)\n",
        "    \n",
        "    # Split dataset\n",
        "    image_ids = list(mapping.keys())\n",
        "    split = int(len(image_ids) * 0.9)\n",
        "    train_ids, test_ids = image_ids[:split], image_ids[split:]\n",
        "    \n",
        "    # Create and train model\n",
        "    model = create_model()\n",
        "    model.fit(\n",
        "        data_generator(train_ids, mapping, features, tokenizer, MAX_CAPTION_LENGTH, VOCAB_SIZE, BATCH_SIZE),\n",
        "        epochs=EPOCHS,\n",
        "        steps_per_epoch=len(train_ids)//BATCH_SIZE\n",
        "    )\n",
        "    \n",
        "    # Save final model\n",
        "    model.save(os.path.join(WORKING_DIR, 'caption_generator.h5'))"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Inference & Evaluation"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption(image_path):\n",
        "    \"\"\"Generate caption for new image\"\"\"\n",
        "    # Load components\n",
        "    model = load_model(os.path.join(WORKING_DIR, 'caption_generator.h5'))\n",
        "    tokenizer = pickle.load(open(os.path.join(WORKING_DIR, 'tokenizer.pkl'), 'rb'))\n",
        "    \n",
        "    # Process image\n",
        "    vgg = VGG16()\n",
        "    vgg = Model(inputs=vgg.inputs, outputs=vgg.layers[-2].output)\n",
        "    image = load_img(image_path, target_size=(224, 224))\n",
        "    image = preprocess_input(img_to_array(image).reshape(1, 224, 224, 3))\n",
        "    features = vgg.predict(image, verbose=0)\n",
        "    \n",
        "    # Generate caption\n",
        "    caption = 'startseq'\n",
        "    for _ in range(MAX_CAPTION_LENGTH):\n",
        "        seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "        seq = pad_sequences([seq], maxlen=MAX_CAPTION_LENGTH)\n",
        "        pred = model.predict([features, seq], verbose=0)\n",
        "        pred_word = idx_to_word(np.argmax(pred), tokenizer)\n",
        "        \n",
        "        if pred_word is None or pred_word == 'endseq':\n",
        "            break\n",
        "        caption += ' ' + pred_word\n",
        "    \n",
        "    return caption.replace('startseq ', '').replace(' endseq', '')"
      ],
      "metadata": {}
    }
  ]
}
